{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Show Code.\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Show Code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.load_session(\"Pixel_Value_Raw_Distance.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE SIZE CONSTANTS\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_HEIGHT = 32\n",
    "TOTAL_PIXELS = IMAGE_WIDTH * IMAGE_HEIGHT\n",
    "\n",
    "#DATASET CONSTANTS\n",
    "DATASET = \"MPEG7\"\n",
    "\n",
    "#TESTING METRIC CONSTANTS\n",
    "K_START_RANGE, K_END_RANGE = 1,20 #Inclusive\n",
    "K_INCREMENT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_folder(folder):\n",
    "    images = []\n",
    "    filenames = []\n",
    "    y = []\n",
    "    category, idx = \"none\", 0\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        if filename.endswith(\".gif\"):\n",
    "            image = Image.open(os.path.join(folder, filename))\n",
    "            image = image.resize((32, 32), Image.ANTIALIAS)\n",
    "            if image is not None:\n",
    "                images.append(image)\n",
    "                filenames.append(filename)\n",
    "                if filename.startswith(category):\n",
    "                    y.append(idx)\n",
    "                else:\n",
    "                    category = filename.split('-')[0]\n",
    "                    idx = idx + 1\n",
    "                    y.append(idx)                    \n",
    "    return images, filenames, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(dataset):\n",
    "    folder = \"\\\\\" + dataset\n",
    "    full_path = os.getcwd() + folder\n",
    "    images, filenames, y = load_image_from_folder(full_path)\n",
    "\n",
    "    return images, filenames, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "#Display Images From Start to Stop\n",
    "def display_images(start, stop):\n",
    "    for i in range(start, stop):\n",
    "        display(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_images_to_2Darr(images, y):\n",
    "    tmp = []\n",
    "    for image in images:\n",
    "        im = np.asarray(image)\n",
    "        im = im.reshape((1024,))\n",
    "        tmp.append(im)\n",
    "    imgs_array = np.array(tmp)\n",
    "    y = np.array(y)\n",
    "    return imgs_array, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_names(filenames):\n",
    "    names = []\n",
    "\n",
    "    for name in filenames:\n",
    "        names.append(name.split('-')[0])\n",
    "\n",
    "    label_names = np.array(names)\n",
    "    return label_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def display_images(k, query,indexes, data, query_text=\"\", label_name=\"\", result_text=\"\", display_query=True):\n",
    "    if (display_query):\n",
    "        print(color.BOLD + color.UNDERLINE + query_text + color.END + \" \" + label_name)\n",
    "        query = [query.reshape((32,32))]\n",
    "        q_output = np.array(query)*255\n",
    "        q_output = q_output.transpose(1, 0, 2)\n",
    "        q_output = q_output.reshape((32, -1))\n",
    "        query_img = Image.fromarray(q_output)\n",
    "        display(query_img)\n",
    "    \n",
    "    if (result_text):\n",
    "        print(color.BOLD + result_text)\n",
    "        \n",
    "    temp = []\n",
    "    for ind in indexes[:k]:\n",
    "        temp.append(data[ind].reshape((32,32)))\n",
    "    output = np.array(temp)*255\n",
    "    output = output.transpose(1, 0, 2)\n",
    "    output = output.reshape((32, -1))\n",
    "    im_query= Image.fromarray(output)\n",
    "    display(im_query) \n",
    "    \n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Images\n",
    "images, filenames, y = load_images(DATASET)\n",
    "#Convert to Numpy Array\n",
    "imgs_array, y = convert_images_to_2Darr(images, y)\n",
    "#Convert Label Names\n",
    "label_names = get_label_names(filenames)\n",
    "#Split Training Data\n",
    "X_train, X_test, y_train, y_test, train_names, test_names = train_test_split(imgs_array, y, label_names, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def calculate_precision_and_recall(index, idx, train_names, test_names, label_amounts, k):\n",
    "    correct_label = test_names[idx]\n",
    "    precisions = [] #used for average precision\n",
    "    found = 0\n",
    "    k_val = 0\n",
    "    recall = -1\n",
    "  \n",
    "    while (found < label_amounts[correct_label]) or (k_val <= k):\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        \n",
    "        \n",
    "        for i in range(0,k_val+1):\n",
    "            if (train_names[index[i]] == correct_label):\n",
    "                true_positives+=1\n",
    "                #Will be equal to precision at last correct in image in range\n",
    "                precision = true_positives/(true_positives + false_positives)\n",
    "                \n",
    "            else:\n",
    "                false_positives+=1\n",
    "                precision = -1\n",
    "                \n",
    "            #if we have just checked k documents, precision is equal to precision@k\n",
    "            #this will keep getting overwritten with the same value but it is not a big deal\n",
    "            if (i == k-1):\n",
    "                precision_at_k = true_positives/(true_positives + false_positives)\n",
    "                #Storing recall@k\n",
    "                recall = true_positives/(label_amounts[correct_label] - true_positives + true_positives)  \n",
    "            \n",
    "        \n",
    "        #If Relevant Document found in this iteration add its precision value \n",
    "        if (precision != -1):\n",
    "            precisions.append(precision)\n",
    "        \n",
    "        #Check if last value was correct, if so update found\n",
    "        if (train_names[index[i]] == correct_label):\n",
    "            found+=1\n",
    "            \n",
    "        k_val+=1\n",
    "    \n",
    "     #If empty add a zero to array for averaging\n",
    "    if not precisions:\n",
    "            precisions.append(0)\n",
    "    \n",
    "    #if we did not change recall, that means all were found before k, therefore recall is 100%\n",
    "    #precision@k will also not change after this point so it can be computed now\n",
    "    if recall == -1:\n",
    "        recall = 1\n",
    "        \n",
    "    #Calculate Average Precision\n",
    "    average_precision = np.average(precisions)\n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    return average_precision, recall, precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "#Sort the test data based on euclidean distance calculated from the training data\n",
    "def calculate_average_precisions(train_data, test_data, train_names, test_names, k=10,image_display_num=0):\n",
    "    images_displayed = 0\n",
    "    label_amounts = pd.value_counts(train_names)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    precisions_at_k = []\n",
    "    \n",
    "    \n",
    "    for idx, query in enumerate(test_data):\n",
    "        #Calculate first k closest iamges\n",
    "        query = query.reshape((1, -1))\n",
    "        D = euclidean_distances(train_data, query).squeeze()\n",
    "        index = np.argsort(D)\n",
    "    \n",
    "    \n",
    "        #Calcualte Metrics for Current Query\n",
    "        average_precision, recall, precision_at_k = calculate_precision_and_recall(index, idx, train_names, test_names, label_amounts, k)\n",
    "            \n",
    "        #Append Results\n",
    "        precisions.append(average_precision)\n",
    "        recalls.append(recall)\n",
    "        precisions_at_k.append(precision_at_k)\n",
    "        \n",
    "        if (images_displayed < image_display_num):\n",
    "            display_images(k, query, index, X_train, \"Query:\", test_names[idx], \"Results:\", True) \n",
    "            print(\"Label Amount \", label_amounts[test_names[idx]])\n",
    "            print(\"Precision: {0:.2f}\".format(average_precision))\n",
    "            print(\"Recall@K {0:.2f}: \".format(recall))\n",
    "            print(\"Precision@K {0:.2f}\".format(precision_at_k))\n",
    "            images_displayed += 1\n",
    "         \n",
    "    return precisions, recalls, precisions_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_mAPs, all_recalls_at_k, all_precisions_at_k, k_labels = [], [], [], []\n",
    "for k_val in range(K_START_RANGE,K_END_RANGE+1):\n",
    "    average_precisions, recalls_at_k, precisions_at_k = calculate_average_precisions(X_train, X_test, train_names, test_names, k=k_val, image_display_num=0)\n",
    "    all_mAPs.append(np.average(average_precisions))\n",
    "    all_recalls_at_k.append(np.average(recalls_at_k))\n",
    "    all_precisions_at_k.append(np.average(precisions_at_k))\n",
    "    k_labels.append(k_val)\n",
    "    \n",
    "overall_mAP = np.average(all_mAPs)\n",
    "overall_precision_at_k = np.average(all_precisions_at_k)\n",
    "overall_recall_at_k = np.average(all_recalls_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(\"<h1><left><u>Pixel Value with Raw Distance: (k={0} to {1})</left></h1><u>\".format(K_START_RANGE, K_END_RANGE)))\n",
    "display(Markdown(\"<h2><left>Pixel Value with Raw Distance mAP: {0:.2f}</left></h2>\".format(overall_mAP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "#Add Overall mAP to bottom\n",
    "col1 = k_labels + ['<b>Overall<b>']\n",
    "col2 = all_precisions_at_k + ['<b>'+str(overall_precision_at_k)+'</b>']\n",
    "col3 = all_recalls_at_k + ['<b>'+str(overall_recall_at_k)+'</b>']\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=['<b>k</b>','<b>Precision@k</b>', '<b>Recall@k</b>'],\n",
    "                line_color='darkslategray',\n",
    "                fill_color='#8aa1b4',\n",
    "                align='left'),\n",
    "    cells=dict(values=[col1, col2,col3], \n",
    "               line_color='darkslategray',\n",
    "               fill_color=['#c4cfd9', '#ffffff'],\n",
    "               align='left'), columnwidth=[150,800])\n",
    "])\n",
    "\n",
    "fig.update_layout(width=900, height=700, title_text=\"<b>Precision and Recall for Different Values of k</b>\", title_x=0.5,title_y=0.91)\n",
    "fig.update_layout(title_font=dict(size=20))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change size of cell output to avoid scrolling\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>div.output_scroll { height: 70em; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "precision_recall_curve = plt.plot(all_recalls_at_k, all_precisions_at_k)\n",
    "plt.title(\"Pixel Value Raw Distance (k={0}-{1})\".format(K_START_RANGE, K_END_RANGE),fontweight='bold')\n",
    "plt.xlabel(\"Recall@k\",fontweight='bold')\n",
    "plt.ylabel(\"Precision@k\",fontweight='bold')\n",
    "plt.xticks([0.2,0.4,0.6,0.8,1.0])\n",
    "plt.yticks([0.2,0.4,0.6,0.8,1.0])\n",
    "plt.rcParams['axes.titlesize'] = 25\n",
    "plt.rc('axes', labelsize=25)    # fontsize of the x and y labels\n",
    "plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "plt.savefig('../Graphs/Pixel_Value_Raw_Distance.png', facecolor='white')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rename and Store Variables to be Presented in Different Notebook\n",
    "raw_distance_overall_mAP = overall_mAP\n",
    "raw_distance_mAPs = all_mAPs\n",
    "raw_distance_total_recalls = all_average_total_recalls\n",
    "\n",
    "%store raw_distance_overall_mAP\n",
    "%store raw_distance_mAPs\n",
    "%store raw_distance_total_recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example with output\n",
    "calculate_average_precisions(X_train, X_test, train_names, test_names, k=2, image_display_num=0)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session(\"Pixel_Value_Raw_Distance.db\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
